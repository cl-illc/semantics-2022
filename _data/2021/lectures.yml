-
  layout: lecture
  selected: y
  date: 2022-04-05
  img: introduction-icon_1-267x300
  uid: intro
  title: "Introduction: Learning word and sentence representations"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: >
    "In this introductory lecture I will give an overview of the course and we will discuss learning word and sentence representations from text."
  background:
  discussion:
  slides: resources/slides/Sem2022-lecture1.pdf
  further: 
   -  "Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. [A large annotated corpus for learning natural language inference](https://arxiv.org/pdf/1508.05326.pdf). arXiv preprint arXiv:1508.05326, 2015."
   - "Alexis Conneau and Douwe Kiela. [Senteval: An evaluation toolkit for universal sentence representations](https://www.aclweb.org/anthology/L18-1269.pdf). In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018), 2018."
   - "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. [Supervised learning of universal sentence representations from natural language inference data](https://arxiv.org/pdf/1705.02364.pdf). arXiv preprint arXiv:1705.02364, 2017."
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2022-04-08
  img: bert
  uid: l2
  title: "Attention and transformers"
  instructor: "Phillip Lippe and Ekaterina Shutova"
  note: 
  abstract: "In this session we will introduce attention and transformer architectures"
  background:
  discussion:
  slides: resources/slides/ATCS_2022_Attention_Mechanisms.pdf
  further:
    - "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. [Attention Is All You Need](https://arxiv.org/abs/1706.03762). In Proceedings of NIPS 2017."
    - "A very helpful [blog post](https://jalammar.github.io/illustrated-transformer/) explaining the transformer architecture."
    - "Visualization of attention heads for BERT: [https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz)"
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: 2022-04-12
  img: Sent-rep
  uid: l3
  title: "Seminar: Sentence representations and contextualised word representations"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss two recent papers on learning general-purpose sentence representations and contextualised word representations."
  background:
  discussion: 
    - "In this session we will discuss the following papers:"
    - "Allen Nie, Erin D. Bennett, Noah D. Goodman. 2019. [DisSent: Learning Sentence Representations from Explicit Discourse Relations](https://www.aclweb.org/anthology/P19-1442.pdf). In Proceedings of ACL 2019."
    - "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee and Luke Zettlemoyer 2018.  [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf). In Proceedings of NAACL 2018, New Orleans, Louisiana."
  slides: 
  further:
    - "Lajanugen Logeswaran and Honglak Lee. 2018. [An efficient framework for learning sentence representations](https://openreview.net/pdf?id=rJvJXZb0W). In Proceedings of ICLR 2018."
    - "Ge Gao, Eunsol Choi, Yejin Choi and Luke Zettlemoyer. 2018. [Neural Metaphor Detection in Context](https://aclweb.org/anthology/D18-1060). In Proceedings of EMNLP 2018, Brussels, Belgium"
    - "Please see Canvas for project descriptions and references to related papers."
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2022-04-19
  img: BERT
  uid: l5
  title: "Seminar: The BERT model"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss the BERT model."
  background:
  discussion: 
    - "In this session we will discuss the following papers:" 
    - "Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. 2019. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf). In Proceedings of NAACL 2019."
    - "Ian Tenney, Dipanjan Das, Ellie Pavlick. 2019.  [BERT Rediscovers the Classical NLP Pipeline](https://www.aclweb.org/anthology/P19-1452/). In Proceedings of ACL 2019."
  slides:
  code: 
  data:   
-
  layout: lecture
  selected: y
  date: 2022-04-20
  img: Multilingual
  uid: l6
  title: "Multilingual models"
  instructor: "Rochelle Choenni and Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss learning multilingual word and sentence representations."
  background:
  further:
    - "The paper that introduced byte pair encoding: Rico Sennrich, Barry Haddow, Alexandra Birch. 2016. [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf). In Proceedings of ACL 2016."
    - "Zirui Wang, Zachary C. Lipton, Yulia Tsvetkov (2020). [On Negative Interference in Multilingual Language Models](https://www.aclweb.org/anthology/2020.emnlp-main.359.pdf). In the proceedings of EMNLP 2020."
    - "Mikel Artetxe and Holger Schwenk. 2018. [Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond.](https://arxiv.org/pdf/1812.10464.pdf) Transactions of the Association for Computational Linguistics."
  discussion: 
  slides: resources/slides/multilingual_modelling.pdf
  further:
  code: 
  data:   
-
  layout: lecture
  selected: y
  date: 2022-04-26
  img: Meta-learning
  uid: l9
  title: "Meta-learning"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session I will introduce the meta-learning framerwork and we will discuss the application of meta-learning to NLP."
  background:
  slides: resources/slides/Sem2022-meta-learning-lecture.pdf
  further:  
    - "The paper by Finn et al (2017) introducing [model-agnostic meta-learning](https://arxiv.org/abs/1703.03400)."
    - "Jake Snell, Kevin Swersky, Richard S. Zemel. 2017 [Prototypical Networks for Few-shot Learning](https://arxiv.org/abs/1703.05175)."
    - "Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, Hugo Larochelle (2019).  [Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples](https://arxiv.org/abs/1903.03096)."
    - "Nithin Holla, Pushkar Mishra, Helen Yannakoudakis and Ekaterina Shutova. 2020. [Learning to Learn to Disambiguate: Meta-Learning for Few-Shot Word Sense Disambiguation.](https://arxiv.org/pdf/2004.14355.pdf) In Findings of EMNLP 2020."
    - "Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, Andrew McCallum. 2020. [Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks](https://arxiv.org/abs/2009.08445)."
    - "Nithin Holla, Pushkar Mishra, Helen Yannakoudakis, Ekaterina Shutova. 2020. [Meta-Learning with Sparse Experience Replay for Lifelong Language Learning.](https://arxiv.org/pdf/2009.04891.pdf) In arXiv e-prints: 2009.04891."
  discussion:  
    - "In this session we will discuss the following papers:"
    - "Nooralahzadeh, F., Bekoulis, G., Bjerva, J., & Augenstein, I. (2020). [Zero-Shot Cross-Lingual Transfer with Meta Learning](https://arxiv.org/pdf/2003.02739.pdf). arXiv preprint arXiv:2003.02739."
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2022-05-10
  img: MTL-NLP
  uid: l7
  title: "Seminar: Multitask learning"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss multitask learning, review several architectures proposed for it and see examples of language processing tasks that can benefit each other through information sharing."
  background:
  discussion: 
    - "In this session we will discuss the following papers:"
    - "Hashimoto, K., Tsuruoka, Y., & Socher, R. (2017). [A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks](https://arxiv.org/pdf/1611.01587.pdf). In Proceedings of EMNLP 2017."
    - "Verna Dankers, Marek Rei, Martha Lewis and Ekaterina Shutova (2019). [Modelling the interplay of metaphor and emotion through multitask learning](https://www.aclweb.org/anthology/D19-1227.pdf). In Proceedings of EMNLP 2019."
  slides:
  further:
  code: 
  data:      
-
  layout: lecture
  selected: y
  date: 2022-05-13
  img: Meta-learning
  uid: l8
  title: "Seminar: Multitask learning and meta-learning"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session we will continue discussing meta-learning."
  background:
  further:
    - "Zirui Wang, Zachary C. Lipton, Yulia Tsvetkov (2020). [On Negative Interference in Multilingual Language Models](https://www.aclweb.org/anthology/2020.emnlp-main.359.pdf). In the proceedings of EMNLP 2020."
    - "Niels van der Heijden, Helen Yannakoudakis, Pushkar Mishra and Ekaterina Shutova. 2021.  [Multilingual and cross-lingual document classification: A meta-learning approach](https://arxiv.org/pdf/2101.11302.pdf). In Proceedings of EACL 2021. Online."
    - "Anna Langedijk, Verna Dankers, Phillip Lippe, Sander Bos, Bryan Cardenas Guevara, Helen Yannakoudakis and Ekaterina Shutova. 2021. [Meta-learning for fast cross-lingual adaptation in dependency parsing](https://arxiv.org/pdf/2104.04736.pdf). In arXiv e-prints: 2104.04736."
    - "Nooralahzadeh, F., Bekoulis, G., Bjerva, J., & Augenstein, I. (2020). [Zero-Shot Cross-Lingual Transfer with Meta Learning](https://arxiv.org/pdf/2003.02739.pdf). arXiv preprint arXiv:2003.02739."
  discussion:   
    - "In this session we will discuss the following papers:"
    - "Jonas Pfeiffer, Aishwarya Kamath, Andreas Ruckle, Kyunghyun Cho, Iryna Gurevych (2021). [AdapterFusion: Non-Destructive Task Composition for Transfer Learning](https://aclanthology.org/2021.eacl-main.39.pdf). In Proceedings of EACL 2021."
    - "Trapit Bansal, Rishikesh Jha, Andrew McCallum, 2019. [Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks](https://arxiv.org/pdf/1911.03863.pdf). Arxiv."
  slides: 
  code: 
  data: 
-
  layout: lecture
  selected: y
  date: 2022-05-17
  img: GNN
  uid: l10
  title: "Graph neural networks in NLP"
  instructor: "Pushkar Mishra and Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss deep learning on graphs and how it has been applied in NLP."
  background:
  discussion: 
  slides: 
  further: 
  code: 
  data:    
-
  layout: lecture
  selected: y
  date: 2022-05-20
  img: GNN
  uid: l11
  title: "Seminar: Model interpretation and task-specific subnetworks"
  instructor: "Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss recent techniques for model interpretation in NLP, such as measuring the importance of individual attention heads and finding task-specific subnetworks in the BERT model."
  background:
  discussion: 
    - "In this session we will discuss the following papers:"
    - "Paul Michel, Omer Levy, Graham Neubig. [Are Sixteen Heads Really Better than One?](https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf) In Proceedings of NeuroIPS 2019."
    - "Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, Michael Carbin. [The Lottery Ticket Hypothesis for Pre-trained BERT Networks.](https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf) In Proceedings of NeuroIPS 2020."
  slides: 
  further:
    - "Tianhe Yu, Saurabh Kumar, Abhishek Gupta, SergeyLevine, Karol Hausman, and Chelsea Finn. (2020). [Gradient surgery for multi-task learning](https://arxiv.org/pdf/2001.06782.pdf). arXivpreprint arXiv:2001.06782."
-
  layout: lecture
  selected: y
  date: 2022-05-24
  img: Discourse
  uid: l10
  title: "Seminar: Bias in NLP models"
  instructor: "Alina Leidinger and Ekaterina Shutova"
  note: 
  abstract: "In this session we will discuss research bias in NLP models."
  background:
  discussion: 
  slides: 
  further: 
  code: 
  data:  
-  
  layout: lecture
  selected: y
  date: 2022-05-31
  img: Poster
  uid: l12
  title: "Project presentations"
  instructor: "Ekaterina Shutova, Rochelle Choenni and Alina Leidinger"
  note: 
  abstract: "In this session, you will present the results of your research projects."
  background:
  discussion:
  slides: 
  code: 
  data: 
